{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dLFpWP3IXl_I",
      "metadata": {
        "id": "dLFpWP3IXl_I"
      },
      "outputs": [],
      "source": [
        "# Import general python libraries\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from skimage import exposure\n",
        "from scipy.ndimage import convolve\n",
        "from time import time\n",
        "import random\n",
        "import random as python_random\n",
        "import tensorflow as tf\n",
        "import sys\n",
        "\n",
        "# Import the GDAL module from the osgeo package\n",
        "from osgeo import gdal\n",
        "\n",
        "# Import necessary functions from scikit-learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# Import necessary functions and classes from Keras\n",
        "from keras.utils import to_categorical\n",
        "from keras.optimizers import Adam\n",
        "#from keras.losses import categorical_crossentropy\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from tensorflow.keras.metrics import BinaryAccuracy, Precision, Recall, IoU, MeanIoU, FalseNegatives, FalsePositives\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, Conv2DTranspose, concatenate, Dropout, BatchNormalization\n",
        "from keras.callbacks import ModelCheckpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57e7bceb",
      "metadata": {
        "id": "57e7bceb"
      },
      "outputs": [],
      "source": [
        "# Define the function for normalisation of vegetation indices\n",
        "def post_idx_calc(index, normalise):\n",
        "    # Replace nan with zero and inf with finite numbers\n",
        "    idx = np.nan_to_num(index)\n",
        "    if normalise:\n",
        "        return cv2.normalize(\n",
        "            idx, None, alpha=0.0, beta=1.0, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
        "    else:\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d94e0b44",
      "metadata": {
        "id": "d94e0b44"
      },
      "outputs": [],
      "source": [
        "# Define function to calculate vegetation indices\n",
        "def calculate_veg_indices(input_img):\n",
        "# Extract the all channels from the input image\n",
        "    RedEdge = input_img[:, :, 3]\n",
        "    nir = input_img[:, :, 4]\n",
        "    red = input_img[:, :, 2]\n",
        "    green = input_img[:, :, 1]\n",
        "    blue = input_img[:, :, 0]\n",
        "\n",
        "# Define all the vegetation indices\n",
        "    # Calculate vegetation indices\n",
        "    ndvi = post_idx_calc((nir - red) / (nir + red),normalise=False)\n",
        "    dvi = post_idx_calc((nir - red),normalise=False)\n",
        "    tvi = post_idx_calc((60*(nir - green)) - (100 * (red - green)),normalise=False)\n",
        "    gdvi = post_idx_calc((nir - green),normalise=False)\n",
        "    endvi = post_idx_calc(((nir + green) - (2 * blue)) / ((nir + green) + (2 * blue)),normalise=False)\n",
        "\n",
        "    veg_indices = np.stack((ndvi, dvi, tvi, gdvi, endvi), axis=2)\n",
        "\n",
        "    return veg_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c64399d",
      "metadata": {
        "id": "8c64399d"
      },
      "outputs": [],
      "source": [
        "# Define a function to get the width and height of an image using GDAL (If required)\n",
        "def get_image_dimensions(file_path):\n",
        "    ds = gdal.Open(file_path)\n",
        "    if ds is not None:\n",
        "        width = ds.RasterXSize\n",
        "        height = ds.RasterYSize\n",
        "        return width, height\n",
        "    return None, None\n",
        "\n",
        "# Minimum width and height for filtering\n",
        "min_width = 256\n",
        "min_height = 256\n",
        "max_width = 2000\n",
        "max_height = 2000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba66c745",
      "metadata": {
        "id": "ba66c745"
      },
      "outputs": [],
      "source": [
        "# Define the tile size and overlap percentage\n",
        "tile_size = 256\n",
        "overlap = int(tile_size * 0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a089c06c",
      "metadata": {
        "id": "a089c06c"
      },
      "outputs": [],
      "source": [
        "# Define the root directory with input images and respective masks\n",
        "root_data_folder = r'/content/drive/MyDrive/SCC/pandanus_classification'\n",
        "root_image_folder = r'/content/drive/MyDrive/SCC/pandanus_classification/msi_image_mask_rois/training'\n",
        "root_model_folder =os.path.join(root_data_folder, 'model&outcomes')\n",
        "# Check if the \"model&outcomes\" folder exists, and create it if it doesn't\n",
        "if not os.path.exists(root_model_folder):\n",
        "    os.makedirs(root_model_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37ab9c18",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37ab9c18",
        "outputId": "4968e2de-32f1-4db9-f15e-be9d7a8dc207"
      },
      "outputs": [],
      "source": [
        "# Store the tiled images and masks\n",
        "image_patches = []\n",
        "mask_patches = []\n",
        "\n",
        "# Define a function to get the width and height of an image using GDAL\n",
        "def get_image_dimensions(file_path):\n",
        "    ds = gdal.Open(file_path)\n",
        "    if ds is not None:\n",
        "        width = ds.RasterXSize\n",
        "        height = ds.RasterYSize\n",
        "        return width, height\n",
        "    return None, None\n",
        "\n",
        "# Specify the folder paths for images and masks\n",
        "image_folder_path = os.path.join(root_image_folder, 'msi_rois')\n",
        "mask_folder_path = os.path.join(root_image_folder, 'msi_mask_rois')\n",
        "\n",
        "# Filter image and mask files based on dimensions\n",
        "filtered_image_files = []\n",
        "filtered_mask_files = []\n",
        "\n",
        "input_img_folder = os.path.join(root_image_folder, 'msi_rois')\n",
        "input_mask_folder = os.path.join(root_image_folder, 'msi_mask_rois')\n",
        "\n",
        "img_files = [file for file in os.listdir(input_img_folder) if file.endswith(\".tif\")]\n",
        "mask_files = [file for file in os.listdir(input_mask_folder) if file.endswith(\".tif\")]\n",
        "\n",
        "# Iterate through the image files\n",
        "for img_file in img_files:\n",
        "    img_path = os.path.join(image_folder_path, img_file)\n",
        "    img_width, img_height = get_image_dimensions(img_path)\n",
        "\n",
        "    if img_width is not None and img_height is not None:\n",
        "        if min_width <= img_width <= max_width and min_height <= img_height <= max_height:\n",
        "            filtered_image_files.append(img_path)\n",
        "\n",
        "# Iterate through the mask files\n",
        "for mask_file in mask_files:\n",
        "    mask_path = os.path.join(mask_folder_path, mask_file)\n",
        "    mask_width, mask_height = get_image_dimensions(mask_path)\n",
        "\n",
        "    if mask_width is not None and mask_height is not None:\n",
        "        if min_width <= mask_width <= max_width and min_height <= mask_height <= max_height:\n",
        "            filtered_mask_files.append(mask_path)\n",
        "\n",
        "# Print the number of filtered image and mask files\n",
        "print(f\"Number of filtered image files: {len(filtered_image_files)}\")\n",
        "print(f\"Number of filtered mask files: {len(filtered_mask_files)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13c4be7e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "13c4be7e",
        "outputId": "bd1fcf1b-cbd8-42ab-dfe9-f3c51b77c581"
      },
      "outputs": [],
      "source": [
        "# Sort the filtered files to ensure consistent ordering\n",
        "filtered_image_files.sort()\n",
        "filtered_mask_files.sort()\n",
        "\n",
        "for i in range(len(filtered_image_files)):\n",
        "    img_file = os.path.basename(filtered_image_files[i])  # Get the file name without the path\n",
        "    mask_file = os.path.basename(filtered_mask_files[i])  # Get the file name without the path\n",
        "\n",
        "    ds_img = gdal.Open(filtered_image_files[i])\n",
        "    ds_mask = gdal.Open(filtered_mask_files[i])\n",
        "    width = ds_img.RasterXSize\n",
        "    height = ds_img.RasterYSize\n",
        "\n",
        "    # Calculate the number of tiles in the image\n",
        "    num_tiles_x = (width - tile_size) // (tile_size - overlap) + 1\n",
        "    num_tiles_y = (height - tile_size) // (tile_size - overlap) + 1\n",
        "\n",
        "    for y in range(num_tiles_y):\n",
        "        for x in range(num_tiles_x):\n",
        "            # Calculate the tile coordinates\n",
        "            x_start = x * (tile_size - overlap)\n",
        "            y_start = y * (tile_size - overlap)\n",
        "            x_end = x_start + tile_size\n",
        "            y_end = y_start + tile_size\n",
        "\n",
        "            # Extract the image tile\n",
        "            input_bands = 5  # Number of input bands\n",
        "            input_img = np.array([ds_img.GetRasterBand(j + 1).ReadAsArray(x_start, y_start, tile_size, tile_size) for j in range(input_bands)])\n",
        "            input_img = np.transpose(input_img, (1, 2, 0))\n",
        "            input_img = exposure.equalize_hist(input_img)\n",
        "\n",
        "            veg_indices = calculate_veg_indices(input_img)\n",
        "            input_img = np.concatenate((input_img, veg_indices), axis=2)\n",
        "\n",
        "            input_mask = ds_mask.GetRasterBand(1).ReadAsArray(x_start, y_start, tile_size, tile_size).astype(int)\n",
        "\n",
        "            image_patches.append(input_img)\n",
        "            mask_patches.append(input_mask)\n",
        "\n",
        "    print(f\"Processed image: {img_file} --> Processed mask: {mask_file}\")\n",
        "\n",
        "# Convert the lists to NumPy arrays\n",
        "image_patches = np.array(image_patches)\n",
        "mask_patches = np.array(mask_patches)\n",
        "\n",
        "# Print the shape of the arrays\n",
        "print(\"image_patches.shape: {}\".format(image_patches.shape))\n",
        "print(\"mask_patches.shape: {}\".format(mask_patches.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f039353",
      "metadata": {
        "id": "3f039353"
      },
      "outputs": [],
      "source": [
        "# This function takes the mask_patches data and converts it into a categorical representation.\n",
        "mask_patches_to_categorical = to_categorical(mask_patches, num_classes=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5eb3559",
      "metadata": {
        "id": "c5eb3559"
      },
      "outputs": [],
      "source": [
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(image_patches, mask_patches_to_categorical, test_size=0.25, random_state=22)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bab880bd",
      "metadata": {
        "id": "bab880bd"
      },
      "outputs": [],
      "source": [
        "#save, print and confirm the model data\n",
        "output_file = os.path.join(root_model_folder, 'trainng and validation samples.txt')\n",
        "# Save the print results to a text file\n",
        "with open(output_file, \"w\") as file:\n",
        "    file.write(\"image_patches.shape: {}\\n\".format(image_patches.shape))\n",
        "    file.write(\"mask_patches.shape: {}\\n\".format(mask_patches.shape))\n",
        "\n",
        "# Save the model data to the text file\n",
        "with open(output_file, \"a\") as file:\n",
        "    file.write(\"\\nX_train shape: {}\\n\".format(X_train.shape))\n",
        "    file.write(\"X_test shape: {}\\n\".format(X_test.shape))\n",
        "    file.write(\"y_train shape: {}\\n\".format(y_train.shape))\n",
        "    file.write(\"y_test shape: {}\\n\".format(y_test.shape))\n",
        "    file.write(\"Image height: {}\\n\".format(X_train.shape[1]))\n",
        "    file.write(\"Image width: {}\\n\".format(X_train.shape[2]))\n",
        "    file.write(\"Image channels: {}\\n\".format(X_train.shape[3]))\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "901c4b35",
      "metadata": {
        "id": "901c4b35"
      },
      "outputs": [],
      "source": [
        "# Apply K-Fold cross validation\n",
        "print (\"Applying K-Fold cross validation...\")\n",
        "start_time = time()\n",
        "\n",
        "cv = KFold(n_splits=10, shuffle=True, random_state=22)\n",
        "\n",
        "# Create a list to store the best model paths and validation loss\n",
        "best_model_paths = []\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "acc_per_fold = [] #save accuracy from each fold\n",
        "loss_per_fold = [] #save accuracy from each fold\n",
        "\n",
        "for fold_no, (train, test) in enumerate(cv.split(X_train, y_train), 1):\n",
        "    print('   ')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    n_classes = 2\n",
        "\n",
        "    def UNet(n_classes, image_height, image_width, image_channels):\n",
        "        inputs = Input((image_height, image_width, image_channels))\n",
        "\n",
        "        seed_value = 22\n",
        "        random.seed(seed_value)\n",
        "        np.random.seed(seed_value)\n",
        "        tf.random.set_seed(seed_value)\n",
        "        python_random.seed(seed_value)\n",
        "\n",
        "        c1 = Conv2D(64, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(inputs)\n",
        "        c1 = BatchNormalization()(c1)\n",
        "        c1 = Dropout(0.2)(c1)\n",
        "        c1 = Conv2D(64, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(c1)\n",
        "        c1 = BatchNormalization()(c1)\n",
        "        p1 = MaxPooling2D((2,2))(c1)\n",
        "\n",
        "        c2 = Conv2D(128, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(p1)\n",
        "        c2 = BatchNormalization()(c2)\n",
        "        c2 = Dropout(0.2)(c2)\n",
        "        c2 = Conv2D(128, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(c2)\n",
        "        c2 = BatchNormalization()(c2)\n",
        "        p2 = MaxPooling2D((2,2))(c2)\n",
        "\n",
        "        c3 = Conv2D(256, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(p2)\n",
        "        c3 = BatchNormalization()(c3)\n",
        "        c3 = Dropout(0.2)(c3)\n",
        "        c3 = Conv2D(256, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(c3)\n",
        "        c3= BatchNormalization()(c3)\n",
        "        p3 = MaxPooling2D((2,2))(c3)\n",
        "\n",
        "        c4 = Conv2D(512, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(p3)\n",
        "        c4 = BatchNormalization()(c4)\n",
        "        c4 = Dropout(0.2)(c4)\n",
        "        c4 = Conv2D(512, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(c4)\n",
        "        c4 = BatchNormalization()(c4)\n",
        "        p4 = MaxPooling2D((2,2))(c4)\n",
        "\n",
        "        c5 = Conv2D(1024, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(p4)\n",
        "        c5 = BatchNormalization()(c5)\n",
        "        c5 = Dropout(0.2)(c5)\n",
        "        c5 = Conv2D(1024, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(c5)\n",
        "        c5 = BatchNormalization()(c5)\n",
        "\n",
        "        u6 = Conv2DTranspose(512, (2,2), strides=(2,2), padding=\"same\")(c5)\n",
        "        u6 = concatenate([u6, c4])\n",
        "        c6 = Conv2D(512, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(u6)\n",
        "        c6 = BatchNormalization()(c6)\n",
        "        c6 = Dropout(0.2)(c6)\n",
        "        c6 = Conv2D(512, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(c6)\n",
        "        c6 = BatchNormalization()(c6)\n",
        "\n",
        "        u7 = Conv2DTranspose(256, (2,2), strides=(2,2), padding=\"same\")(c6)\n",
        "        u7 = concatenate([u7, c3])\n",
        "        c7 = Conv2D(256, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(u7)\n",
        "        c7 = BatchNormalization()(c7)\n",
        "        c7 = Dropout(0.2)(c7)\n",
        "        c7 = Conv2D(256, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(c7)\n",
        "        c7 = BatchNormalization()(c7)\n",
        "\n",
        "        u8 = Conv2DTranspose(128, (2,2), strides=(2,2), padding=\"same\")(c7)\n",
        "        u8 = concatenate([u8, c2])\n",
        "        c8 = Conv2D(128, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(u8)\n",
        "        c8 = BatchNormalization()(c8)\n",
        "        c8 = Dropout(0.2)(c8)\n",
        "        c8 = Conv2D(128, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(c8)\n",
        "        c8 = BatchNormalization()(c8)\n",
        "\n",
        "        u9 = Conv2DTranspose(64, (2,2), strides=(2,2), padding=\"same\")(c8)\n",
        "        u9 = concatenate([u9, c1], axis=3)\n",
        "        c9 = Conv2D(64, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(u9)\n",
        "        c9 = BatchNormalization()(c9)\n",
        "        c9 = Dropout(0.2)(c9)\n",
        "        c9 = Conv2D(64, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(c9)\n",
        "        c9 = BatchNormalization()(c9)\n",
        "\n",
        "        outputs = Conv2D(n_classes, (1,1), activation=\"softmax\")(c9)\n",
        "\n",
        "        model = Model(inputs=inputs, outputs=outputs)\n",
        "        return model\n",
        "    #----------------------------------------------------------------------#\n",
        "    # Create the model\n",
        "    image_height = X_train.shape[1]\n",
        "    image_width = X_train.shape[2]\n",
        "    image_channels = X_train.shape[3]\n",
        "    model=UNet(n_classes=n_classes,\n",
        "                            image_height=image_height,\n",
        "                            image_width=image_width,\n",
        "                            image_channels=image_channels)\n",
        "    #----------------------------------------------------------------------#\n",
        "    #Complie the model\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        loss=BinaryCrossentropy(),\n",
        "        metrics=[BinaryAccuracy(),Precision(class_id=1), Recall(class_id=1), IoU(num_classes=2,target_class_ids=[1]), MeanIoU(num_classes=2), FalseNegatives(), FalsePositives()])\n",
        "\n",
        "    # Specify the filepath for where to save the weights for the best model\n",
        "    best_model_path = os.path.join(root_model_folder, f'save_best_model_fold_{fold_no}.hdf5')\n",
        "\n",
        "    # Create a ModelCheckpoint for the best model based on validation loss\n",
        "    checkpoint_best_model = ModelCheckpoint(\n",
        "        best_model_path,\n",
        "        save_best_only=True,\n",
        "        monitor='val_loss',\n",
        "        mode='min'\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(X_train, y_train,\n",
        "                    batch_size=30,\n",
        "                    verbose=1,\n",
        "                    epochs=200,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    shuffle=True,\n",
        "                    callbacks=[checkpoint_best_model])\n",
        "\n",
        "    best_model_paths.append(best_model_path)\n",
        "\n",
        "\n",
        "    # Evaluate the model - report accuracy and capture it into a list for future reporting\n",
        "    scores = model.evaluate(X_test, y_test, verbose=1)\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "\n",
        "    scores = model.evaluate(X_test, y_test, verbose=1)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no = fold_no + 1\n",
        "\n",
        "# Initialize a list to store fold numbers and corresponding accuracies and loss\n",
        "fold_and_acc_list = [(fold_no, acc) for fold_no, acc in enumerate(acc_per_fold, 1)]\n",
        "fold_and_loss_list = [(fold_no, loss) for fold_no, loss in enumerate(loss_per_fold, 1)]\n",
        "\n",
        "# Calculate the average val_binary_accuracy\n",
        "average_accuracy = sum(acc_per_fold) / len(acc_per_fold)\n",
        "\n",
        "# Calculate the average val_binary_accuracy\n",
        "average_loss = sum(loss_per_fold) / len(acc_per_fold)\n",
        "\n",
        "for fold_no, acc in enumerate(acc_per_fold, 1):\n",
        "    print(f'Fold {fold_no} val_binary_accuracy: {acc}\\n\"')\n",
        "\n",
        "for fold_no, acc in enumerate(loss_per_fold, 1):\n",
        "    print(f'Fold {fold_no} val_binary_loss: {acc}\\n\"')\n",
        "\n",
        "print(f\"Average val_binary_accuracy across all folds: {average_accuracy}\\n\")\n",
        "print(f\"Average val_binary_loss across all folds: {average_loss}\\n\")\n",
        "\n",
        "# Calculate and print the training time\n",
        "end_time = time()\n",
        "training_time = end_time - start_time\n",
        "print(f\"Training time: {training_time} seconds\")\n",
        "\n",
        "# Export confusion matrix and classification report as .txt\n",
        "file_path = os.path.join(root_model_folder, 'K_Fold_outcome report.txt')\n",
        "with open(file_path, 'w') as file:\n",
        "    file.write(f\"Training Time: {training_time} seconds\\n\")\n",
        "    for fold_no, acc in fold_and_acc_list:\n",
        "        file.write(f'Fold {fold_no} val_binary_accuracy: {acc}\\n')\n",
        "\n",
        "    for fold_no, loss in fold_and_loss_list:\n",
        "        file.write(f'Fold {fold_no} val_binary_loss: {loss}\\n')\n",
        "\n",
        "    file.write(f\"Average val_binary_accuracy across all folds: {average_accuracy}\\n\")\n",
        "    file.write(f\"Average val_binary_loss across all folds: {average_loss}\\n\")\n",
        "print('K_Fold_outcome report')\n",
        "\n",
        "\n",
        "# Initialize fold numbers\n",
        "fold_numbers = list(range(1, len(acc_per_fold) + 1))\n",
        "\n",
        "# Plot the accuracy\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.bar(fold_numbers, acc_per_fold, color='green')\n",
        "plt.title('Validation Accuracy vs. Fold Number')\n",
        "plt.xlabel('Fold Number')\n",
        "plt.ylabel('val_accuracy(%)')\n",
        "plt.xticks(fold_numbers)\n",
        "plt.axhline(y=average_accuracy, color='red', linestyle='--', label=f'Average Accuracy(%): {average_accuracy:.2f}')\n",
        "plt.legend(loc='upper right')\n",
        "plt.savefig(os.path.join(root_model_folder, 'K_Fold_accuracy.png'), bbox_inches='tight')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "print('Saved K_Fold Accuracy graph')\n",
        "\n",
        "# Create a bar chart for binary loss\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.bar(fold_numbers, loss_per_fold, color='blue')\n",
        "plt.title('Validation Loss vs. Fold Number')\n",
        "plt.xlabel('Fold Number')\n",
        "plt.ylabel('val_loss')\n",
        "plt.xticks(fold_numbers)\n",
        "plt.axhline(y=average_loss, color='red', linestyle='--', label=f'Average Loss: {average_loss:.2f}')\n",
        "plt.legend(loc='upper right')\n",
        "plt.savefig(os.path.join(root_model_folder, 'K-Fold_loss.png'), bbox_inches='tight')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "print('Saved K-Fold_loss graph')\n",
        "\n",
        "print (\"Done K-Fold cross validation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "428116bc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate the standard deviation of val_binary_accuracy and val_binary_loss\n",
        "accuracy_std = np.std(acc_per_fold)\n",
        "loss_std = np.std(loss_per_fold)\n",
        "\n",
        "# Print the standard deviation\n",
        "print(f\"Standard Deviation of val_binary_accuracy: {accuracy_std:.2f}\\n\")\n",
        "print(f\"Standard Deviation of val_binary_loss: {loss_std:.2f}\\n\")\n",
        "\n",
        "with open(file_path, 'a') as file:\n",
        "    file.write(f\"Standard Deviation of val_binary_accuracy: {accuracy_std}\\n\")\n",
        "    file.write(f\"Standard Deviation of val_binary_loss: {average_loss}\\n\")\n",
        "\n",
        "# Plot the accuracy with both average and standard deviation lines\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.bar(fold_numbers, acc_per_fold, color='green', label='Accuracy')\n",
        "plt.axhline(y=average_accuracy, color='red', linestyle='--', label=f'Average Accuracy(%): {average_accuracy:.2f}')\n",
        "plt.errorbar(fold_numbers, acc_per_fold, yerr=accuracy_std, linestyle='None', color='black', capsize=5, label='Std Deviation')\n",
        "plt.title('Validation Accuracy vs. Fold Number')\n",
        "plt.xlabel('Fold Number')\n",
        "plt.ylabel('val_accuracy(%)')\n",
        "plt.xticks(fold_numbers)\n",
        "plt.legend(loc='upper right')\n",
        "plt.savefig(os.path.join(root_model_folder, 'K_Fold_accuracy_std.png'), bbox_inches='tight')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "print('Saved K_Fold Accuracy with std graph')\n",
        "\n",
        "# Plot the loss with both average and standard deviation lines\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.bar(fold_numbers, loss_per_fold, color='blue', label='Loss')\n",
        "plt.axhline(y=average_loss, color='red', linestyle='--', label=f'Average Loss: {average_loss:.2f}')\n",
        "plt.errorbar(fold_numbers, loss_per_fold, yerr=loss_std, linestyle='None', color='black', capsize=5, label='Std Deviation')\n",
        "plt.title('Validation Loss vs. Fold Number')\n",
        "plt.xlabel('Fold Number')\n",
        "plt.ylabel('val_loss')\n",
        "plt.xticks(fold_numbers)\n",
        "plt.legend(loc='upper right')\n",
        "plt.savefig(os.path.join(root_model_folder, 'K-Fold_loss_std.png'), bbox_inches='tight')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "print('Saved K-Fold_loss graph')\n",
        "\n",
        "print(\"Done K-Fold cross validation with std graph\")\n",
        "#-------------------------xxxxxx---------------------------------------#"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
