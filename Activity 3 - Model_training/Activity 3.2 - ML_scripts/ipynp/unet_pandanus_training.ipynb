{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c418c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import general python libraries\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from skimage import exposure\n",
    "from scipy.ndimage import convolve\n",
    "from time import time\n",
    "import random\n",
    "import random as python_random\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import the GDAL module from the osgeo package\n",
    "from osgeo import gdal\n",
    "\n",
    "# Import necessary functions from scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Import necessary functions and classes from Keras\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import BinaryAccuracy, Precision, Recall, IoU, MeanIoU, FalseNegatives, FalsePositives\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Conv2DTranspose, concatenate, Dropout, BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6985de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function for normalisation of vegetation indices\n",
    "def post_idx_calc(index, normalise):\n",
    "    # Replace nan with zero and inf with finite numbers\n",
    "    idx = np.nan_to_num(index)\n",
    "    if normalise:\n",
    "        return cv2.normalize(\n",
    "            idx, None, alpha=0.0, beta=1.0, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "    else:\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da11e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to calculate vegetation indices\n",
    "def calculate_veg_indices(input_img):\n",
    "# Extract the all channels from the input image\n",
    "    RedEdge = input_img[:, :, 3]\n",
    "    nir = input_img[:, :, 4]\n",
    "    red = input_img[:, :, 2]\n",
    "    green = input_img[:, :, 1]\n",
    "    blue = input_img[:, :, 0]\n",
    "\n",
    "# Define all the vegetation indices\n",
    "    # Calculate vegetation indices\n",
    "    ndvi = post_idx_calc((nir - red) / (nir + red),normalise=False)\n",
    "    dvi = post_idx_calc((nir - red),normalise=False)\n",
    "    tvi = post_idx_calc((60*(nir - green)) - (100 * (red - green)),normalise=False)\n",
    "    gdvi = post_idx_calc((nir - green),normalise=False)\n",
    "    endvi = post_idx_calc(((nir + green) - (2 * blue)) / ((nir + green) + (2 * blue)),normalise=False)\\\n",
    "        \n",
    "    veg_indices = np.stack((ndvi, dvi, tvi, gdvi, endvi), axis=2)\n",
    "\n",
    "    return veg_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90956cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to get the width and height of an image using GDAL (If required)\n",
    "def get_image_dimensions(file_path):\n",
    "    ds = gdal.Open(file_path)\n",
    "    if ds is not None:\n",
    "        width = ds.RasterXSize\n",
    "        height = ds.RasterYSize\n",
    "        return width, height\n",
    "    return None, None\n",
    "\n",
    "# Minimum and Maximum width and height for filtering\n",
    "min_width = 256\n",
    "min_height = 256\n",
    "max_width = 2000\n",
    "max_height = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a58cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tile size and overlap percentage\n",
    "tile_size = 256\n",
    "overlap = int(tile_size * 0.3)\n",
    "\n",
    "num_classes=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc54697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the root directory with input images and respective masks\n",
    "root_data_folder = r'/home/n10837647/pandanus_classification'\n",
    "root_image_folder = r'/home/n10837647/pandanus_classification/msi_image_mask_rois/training'\n",
    "root_model_folder =os.path.join(root_data_folder, 'model&outcomes')\n",
    "# Check if the \"model&outcomes\" folder exists, and create it if it doesn't\n",
    "if not os.path.exists(root_model_folder):\n",
    "    os.makedirs(root_model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26f6b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the tiled images and masks\n",
    "image_patches = []\n",
    "mask_patches = []\n",
    "\n",
    "# # Define a function to get the width and height of an image using GDAL\n",
    "# def get_image_dimensions(file_path):\n",
    "#     ds = gdal.Open(file_path)\n",
    "#     if ds is not None:\n",
    "#         width = ds.RasterXSize\n",
    "#         height = ds.RasterYSize\n",
    "#         return width, height\n",
    "#     return None, None\n",
    "\n",
    "# Specify the folder paths for images and masks\n",
    "image_folder_path = os.path.join(root_image_folder, 'msi_rois')\n",
    "mask_folder_path = os.path.join(root_image_folder, 'msi_mask_rois')\n",
    "\n",
    "# Filter image and mask files based on dimensions\n",
    "filtered_image_files = []\n",
    "filtered_mask_files = []\n",
    "\n",
    "input_img_folder = os.path.join(root_image_folder, 'msi_rois')\n",
    "input_mask_folder = os.path.join(root_image_folder, 'msi_mask_rois')\n",
    "\n",
    "img_files = [file for file in os.listdir(input_img_folder) if file.endswith(\".tif\")]\n",
    "mask_files = [file for file in os.listdir(input_mask_folder) if file.endswith(\".tif\")]\n",
    "\n",
    "# Iterate through the image files\n",
    "for img_file in img_files:\n",
    "    img_path = os.path.join(image_folder_path, img_file)\n",
    "    img_width, img_height = get_image_dimensions(img_path)\n",
    "    \n",
    "    if img_width is not None and img_height is not None:\n",
    "        if min_width <= img_width <= max_width and min_height <= img_height <= max_height:\n",
    "            filtered_image_files.append(img_path)\n",
    "\n",
    "# Iterate through the mask files\n",
    "for mask_file in mask_files:\n",
    "    mask_path = os.path.join(mask_folder_path, mask_file)\n",
    "    mask_width, mask_height = get_image_dimensions(mask_path)\n",
    "    \n",
    "    if mask_width is not None and mask_height is not None:\n",
    "        if min_width <= mask_width <= max_width and min_height <= mask_height <= max_height:\n",
    "            filtered_mask_files.append(mask_path)\n",
    "\n",
    "# Print the number of filtered image and mask files\n",
    "print(f\"Number of filtered image files: {len(filtered_image_files)}\")\n",
    "print(f\"Number of filtered mask files: {len(filtered_mask_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2624c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the filtered files to ensure consistent ordering\n",
    "filtered_image_files.sort()\n",
    "filtered_mask_files.sort()\n",
    "\n",
    "for i in range(len(filtered_image_files)):\n",
    "    img_file = os.path.basename(filtered_image_files[i])  # Get the file name without the path\n",
    "    mask_file = os.path.basename(filtered_mask_files[i])  # Get the file name without the path\n",
    "    \n",
    "    ds_img = gdal.Open(filtered_image_files[i])\n",
    "    ds_mask = gdal.Open(filtered_mask_files[i])\n",
    "    width = ds_img.RasterXSize\n",
    "    height = ds_img.RasterYSize\n",
    "\n",
    "    # Calculate the number of tiles in the image\n",
    "    num_tiles_x = (width - tile_size) // (tile_size - overlap) + 1\n",
    "    num_tiles_y = (height - tile_size) // (tile_size - overlap) + 1\n",
    "\n",
    "    for y in range(num_tiles_y):\n",
    "        for x in range(num_tiles_x):\n",
    "            # Calculate the tile coordinates\n",
    "            x_start = x * (tile_size - overlap)\n",
    "            y_start = y * (tile_size - overlap)\n",
    "            x_end = x_start + tile_size\n",
    "            y_end = y_start + tile_size\n",
    "\n",
    "            # Extract the image tile\n",
    "            input_bands = 5  # Number of input bands\n",
    "            input_img = np.array([ds_img.GetRasterBand(j + 1).ReadAsArray(x_start, y_start, tile_size, tile_size) for j in range(input_bands)])\n",
    "            input_img = np.transpose(input_img, (1, 2, 0))\n",
    "            input_img = exposure.equalize_hist(input_img)\n",
    "            \n",
    "            veg_indices = calculate_veg_indices(input_img)\n",
    "            input_img = np.concatenate((input_img, veg_indices), axis=2)\n",
    "\n",
    "            input_mask = ds_mask.GetRasterBand(1).ReadAsArray(x_start, y_start, tile_size, tile_size).astype(int)\n",
    "           \n",
    "            image_patches.append(input_img)\n",
    "            mask_patches.append(input_mask)\n",
    "\n",
    "    print(f\"Processed image: {img_file} --> Processed mask: {mask_file}\")\n",
    "\n",
    "# Convert the lists to NumPy arrays\n",
    "image_patches = np.array(image_patches)\n",
    "mask_patches = np.array(mask_patches)\n",
    "\n",
    "# Print the shape of the arrays\n",
    "print(\"image_patches.shape: {}\".format(image_patches.shape))\n",
    "print(\"mask_patches.shape: {}\".format(mask_patches.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4d4052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes the mask_patches data and converts it into a categorical representation. \n",
    "mask_patches_to_categorical = to_categorical(mask_patches, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dfea0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(image_patches, mask_patches_to_categorical, test_size=0.25, random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f773f7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save, print and confirm the model data\n",
    "output_file = os.path.join(root_model_folder, 'trainng and validation samples.txt')\n",
    "# Save the print results to a text file\n",
    "with open(output_file, \"w\") as file:\n",
    "    file.write(\"image_patches.shape: {}\\n\".format(image_patches.shape))\n",
    "    file.write(\"mask_patches.shape: {}\\n\".format(mask_patches.shape))\n",
    "\n",
    "# Save the model data to the text file\n",
    "with open(output_file, \"a\") as file:\n",
    "    file.write(\"\\nX_train shape: {}\\n\".format(X_train.shape))\n",
    "    file.write(\"X_test shape: {}\\n\".format(X_test.shape))\n",
    "    file.write(\"y_train shape: {}\\n\".format(y_train.shape))\n",
    "    file.write(\"y_test shape: {}\\n\".format(y_test.shape))\n",
    "    file.write(\"image height: {}\\n\".format(X_train.shape[1]))\n",
    "    file.write(\"Image width: {}\\n\".format(X_train.shape[2]))\n",
    "    file.write(\"Image channels: {}\\n\".format(X_train.shape[3]))\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a99faa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------\"\"\"**Build the model**\"\"\"----------------------------#\n",
    "# Import module \n",
    "# Define the number of classess (ID 1,2)\n",
    "n_classes = 2\n",
    "\n",
    "def UNet(n_classes, image_height, image_width, image_channels):\n",
    "  inputs = Input((image_height, image_width, image_channels))\n",
    "\n",
    "  seed_value = 22\n",
    "  random.seed(seed_value)\n",
    "  np.random.seed(seed_value)\n",
    "  tf.random.set_seed(seed_value)\n",
    "  python_random.seed(seed_value)\n",
    "  \n",
    "  c1 = Conv2D(64, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(inputs)\n",
    "  c1 = BatchNormalization()(c1)\n",
    "  c1 = Dropout(0.2)(c1)\n",
    "  c1 = Conv2D(64, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(c1)\n",
    "  c1 = BatchNormalization()(c1)\n",
    "  p1 = MaxPooling2D((2,2))(c1)\n",
    "\n",
    "  c2 = Conv2D(128, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(p1)\n",
    "  c2 = BatchNormalization()(c2)\n",
    "  c2 = Dropout(0.2)(c2)\n",
    "  c2 = Conv2D(128, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(c2)\n",
    "  c2 = BatchNormalization()(c2)\n",
    "  p2 = MaxPooling2D((2,2))(c2)\n",
    "\n",
    "  c3 = Conv2D(256, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(p2)\n",
    "  c3 = BatchNormalization()(c3)\n",
    "  c3 = Dropout(0.2)(c3)\n",
    "  c3 = Conv2D(256, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(c3)\n",
    "  c3= BatchNormalization()(c3)\n",
    "  p3 = MaxPooling2D((2,2))(c3)\n",
    "\n",
    "  # c4 = Conv2D(512, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(p3)\n",
    "  # c4 = BatchNormalization()(c4)\n",
    "  # c4 = Dropout(0.2)(c4)\n",
    "  # c4 = Conv2D(512, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(c4)\n",
    "  # c4 = BatchNormalization()(c4)\n",
    "  # p4 = MaxPooling2D((2,2))(c4)\n",
    "\n",
    "  # c5 = Conv2D(1024, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(p4)\n",
    "  # c5 = BatchNormalization()(c5)\n",
    "  # c5 = Dropout(0.2)(c5)\n",
    "  # c5 = Conv2D(1024, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(c5)\n",
    "  # c5 = BatchNormalization()(c5)\n",
    "\n",
    "  c5 = Conv2D(512, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(p4)\n",
    "  c5 = BatchNormalization()(c5)\n",
    "  c5 = Dropout(0.2)(c5)\n",
    "  c5 = Conv2D(512, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(c5)\n",
    "  c5 = BatchNormalization()(c5)\n",
    "\n",
    "  # u6 = Conv2DTranspose(512, (2,2), strides=(2,2), padding=\"same\")(c5)\n",
    "  # u6 = concatenate([u6, c4])\n",
    "  # c6 = Conv2D(512, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(u6)\n",
    "  # c6 = BatchNormalization()(c6)\n",
    "  # c6 = Dropout(0.2)(c6)\n",
    "  # c6 = Conv2D(512, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(c6)\n",
    "  # c6 = BatchNormalization()(c6)\n",
    "\n",
    "  u7 = Conv2DTranspose(256, (2,2), strides=(2,2), padding=\"same\")(c6)\n",
    "  u7 = concatenate([u7, c3])\n",
    "  c7 = Conv2D(256, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(u7)\n",
    "  c7 = BatchNormalization()(c7)\n",
    "  c7 = Dropout(0.2)(c7)\n",
    "  c7 = Conv2D(256, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(c7)\n",
    "  c7 = BatchNormalization()(c7)\n",
    "\n",
    "  u8 = Conv2DTranspose(128, (2,2), strides=(2,2), padding=\"same\")(c7)\n",
    "  u8 = concatenate([u8, c2])\n",
    "  c8 = Conv2D(128, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(u8)\n",
    "  c8 = BatchNormalization()(c8)\n",
    "  c8 = Dropout(0.2)(c8)\n",
    "  c8 = Conv2D(128, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(c8)\n",
    "  c8 = BatchNormalization()(c8)\n",
    "\n",
    "  u9 = Conv2DTranspose(64, (2,2), strides=(2,2), padding=\"same\")(c8)\n",
    "  u9 = concatenate([u9, c1], axis=3)\n",
    "  c9 = Conv2D(64, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(u9)\n",
    "  c9 = BatchNormalization()(c9)\n",
    "  c9 = Dropout(0.2)(c9)\n",
    "  c9 = Conv2D(64, (3,3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(c9)\n",
    "  c9 = BatchNormalization()(c9)\n",
    "\n",
    "  outputs = Conv2D(n_classes, (1,1), activation=\"softmax\")(c9)\n",
    "\n",
    "  model = Model(inputs=inputs, outputs=outputs)\n",
    "  return model\n",
    "#----------------------------------------------------------------------#\n",
    "# Create the model\n",
    "image_height = X_train.shape[1]\n",
    "image_width = X_train.shape[2]\n",
    "image_channels = X_train.shape[3]\n",
    "model=UNet(n_classes=n_classes, \n",
    "                          image_height=image_height, \n",
    "                          image_width=image_width, \n",
    "                          image_channels=image_channels)\n",
    "#----------------------------------------------------------------------#\n",
    "#Complie the model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss=BinaryCrossentropy(),  # Use BinaryCrossentropy for binary classification\n",
    "    metrics=[BinaryAccuracy(),Precision(class_id=1), Recall(class_id=1), IoU(num_classes=2,target_class_ids=[1]), MeanIoU(num_classes=2), FalseNegatives(), FalsePositives()])\n",
    "#----------------------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa0c4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training\n",
    "#----------------------------------------------------------------------#\n",
    "# Define a log directory for checkpoins\n",
    "log_dir = os.path.join(root_model_folder, 'log')  # Create the log directory\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "#----------------------------------------------------------------------#\n",
    "# specify the filepath for where to save the weights\n",
    "weight_path = os.path.join(log_dir, \"weights.{epoch:02d}-{val_loss:.2f}.hdf5\")\n",
    "best_model_path = os.path.join(root_model_folder, 'save_best_model.hdf5')\n",
    "#----------------------------------------------------------------------#\n",
    "# create a ModelCheckpoint for best model\n",
    "checkpoint_best_model = ModelCheckpoint(best_model_path, save_best_only=True, monitor='val_loss', mode='min')\n",
    "#----------------------------------------------------------------------#\n",
    "# create a ModelCheckpoint for save weights\n",
    "checkpoint_weight = ModelCheckpoint(weight_path, ave_weights_only=True, verbose=1, period=50)\n",
    "#----------------------------------------------------------------------#\n",
    "# Start recording time\n",
    "start_time = time()\n",
    "\n",
    "# Train the model with class weights\n",
    "history = model.fit(X_train, y_train, \n",
    "                    batch_size=20, \n",
    "                    verbose=1,\n",
    "                    epochs=100,\n",
    "                    validation_data=(X_test, y_test), \n",
    "                    callbacks=[checkpoint_best_model, checkpoint_weight],\n",
    "                    shuffle=True)\n",
    "\n",
    "# Calculate and print the training time\n",
    "end_time = time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Training time: {training_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c5e1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion_matrix and Classification_report\n",
    "#----------------#\n",
    "# Confusion_matrix\n",
    "#----------------#\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Convert the predicted and true masks to class labels\n",
    "y_pred_classes = np.argmax(y_pred, axis=-1)\n",
    "y_test_classes = np.argmax(y_test, axis=-1)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_test_classes.ravel(), y_pred_classes.ravel())\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(cm)\n",
    "# #----------------------------------------------------------------------#\n",
    "# Plot the confusion matrix \n",
    "labels = ['Background', 'Pandanus']\n",
    "# Plot the confusion matrix using heatmap()\n",
    "plt.figure()\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='d', xticklabels=labels, yticklabels=labels)\n",
    "plt.title('confusion matrix_heatmap')\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.savefig(os.path.join(root_model_folder, 'CM_heatmap.png'), bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved confusion matrix_heatmap')\n",
    "#----------------------------------------------------------------------#\n",
    "#---------------------#\n",
    "# classification report\n",
    "#---------------------#\n",
    "cr = classification_report(y_test_classes.ravel(), y_pred_classes.ravel(), target_names=['Background', 'Pandanus'])\n",
    "# Print the classification report\n",
    "print(cr)\n",
    "#----------------------------------------------------------------------#\n",
    "# Export confusion matrix and classification report as .txt\n",
    "file_path = os.path.join(root_model_folder, 'model training performance report.txt')\n",
    "with open(file_path, 'w') as file:\n",
    "    file.write(f\"Training Time: {training_time} seconds\\n\")\n",
    "    file.write(\"Confusion Matrix:\\n\")\n",
    "    file.write(str(cm))\n",
    "    file.write(\"\\n\\n\")\n",
    "    file.write(\"Classification Report:\\n\")\n",
    "    file.write(cr)\n",
    "print('Saved classification_and_confusion_report')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d28f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export training_history (If necessary)\n",
    "# Create a DataFrame from the history\n",
    "history_df = pd.DataFrame(history.history)\n",
    "# Save the DataFrame to a CSV file\n",
    "history_df.to_csv(os.path.join(root_model_folder,'training_history.csv'), index=False)\n",
    "print('Saved training_history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8620105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot binary_accuracy graphs using history\n",
    "num_epochs = len(history.history['binary_accuracy'])\n",
    "# Plot the accuracy\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(range(1, num_epochs + 1), history.history['binary_accuracy'])\n",
    "plt.plot(range(1, num_epochs + 1), history.history['val_binary_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.savefig(os.path.join(root_model_folder, 'Accuracy.png'), bbox_inches='tight')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print('Saved accuracy graph')\n",
    "#----------------------------------------------------------------------#\n",
    "# plot loss graphs using history\n",
    "num_epochs = len(history.history['loss'])\n",
    "# Plot the accuracy\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(range(1, num_epochs + 1), history.history['loss'])\n",
    "plt.plot(range(1, num_epochs + 1), history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('LOss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.savefig(os.path.join(root_model_folder, 'loss.png'), bbox_inches='tight')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print('Saved loss graph')\n",
    "#----------------------------------------------------------------------#\n",
    "# plot precision graphs using history\n",
    "num_epochs = len(history.history['precision'])\n",
    "# Plot the accuracy\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(range(1, num_epochs + 1), history.history['precision'])\n",
    "plt.plot(range(1, num_epochs + 1), history.history['val_precision'])\n",
    "plt.title('Model precision')\n",
    "plt.ylabel('precision(class id=1)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.savefig(os.path.join(root_model_folder, 'precision.png'), bbox_inches='tight')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print('Saved precision graph')\n",
    "#----------------------------------------------------------------------#\n",
    "# plot recall graphs using history\n",
    "num_epochs = len(history.history['recall'])\n",
    "# Plot the accuracy\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(range(1, num_epochs + 1), history.history['recall'])\n",
    "plt.plot(range(1, num_epochs + 1), history.history['val_recall'])\n",
    "plt.title('Model recall')\n",
    "plt.ylabel('recall(class id=1)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.savefig(os.path.join(root_model_folder, 'recall.png'), bbox_inches='tight')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print('Saved recall graph')\n",
    "#----------------------------------------------------------------------#\n",
    "# plot iou graphs using history\n",
    "num_epochs = len(history.history['io_u'])\n",
    "# Plot the accuracy\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(range(1, num_epochs + 1), history.history['io_u'])\n",
    "plt.plot(range(1, num_epochs + 1), history.history['val_io_u'])\n",
    "plt.title('Model IoU')\n",
    "plt.ylabel('IoU(class id=1)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.savefig(os.path.join(root_model_folder, 'IoU.png'), bbox_inches='tight')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print('Saved IoU graph')\n",
    "#----------------------------------------------------------------------#\n",
    "# plot mean_iou graphs using history\n",
    "num_epochs = len(history.history['mean_io_u'])\n",
    "# Plot the accuracy\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(range(1, num_epochs + 1), history.history['mean_io_u'])\n",
    "plt.plot(range(1, num_epochs + 1), history.history['val_mean_io_u'])\n",
    "plt.title('Model MeanIoU')\n",
    "plt.ylabel('MeanIoU')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.savefig(os.path.join(root_model_folder, 'MeanIoU.png'), bbox_inches='tight')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print('Saved MeanIoU graph')\n",
    "#----------------------------------------------------------------------#\n",
    "# plot false_negatives graphs using history\n",
    "num_epochs = len(history.history['false_negatives'])\n",
    "# Plot the accuracy\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(range(1, num_epochs + 1), history.history['false_negatives'])\n",
    "plt.plot(range(1, num_epochs + 1), history.history['val_false_negatives'])\n",
    "plt.title('Model FalseNegatives')\n",
    "plt.ylabel('FalseNegatives')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.savefig(os.path.join(root_model_folder, 'FalseNegatives.png'), bbox_inches='tight')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print('Saved FalseNegatives graph')\n",
    "#----------------------------------------------------------------------#\n",
    "# plot false_positives graphs using history\n",
    "num_epochs = len(history.history['false_positives'])\n",
    "# Plot the accuracy\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(range(1, num_epochs + 1), history.history['false_positives'])\n",
    "plt.plot(range(1, num_epochs + 1), history.history['val_false_positives'])\n",
    "plt.title('Model FalsePositives')\n",
    "plt.ylabel('FalsePositives')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.savefig(os.path.join(root_model_folder, 'FalsePositives.png'), bbox_inches='tight')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print('Saved FalsePositives graph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465c241d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IOU\n",
    "# Calculate and save IoU for each class\n",
    "class_iou = []\n",
    "with open(file_path, 'a') as file:\n",
    "    file.write(\"\\n\\nIoU Results:\\n\")\n",
    "    for i in range(2):\n",
    "        true_class = (y_test_classes == i)\n",
    "        pred_class = (y_pred_classes == i)\n",
    "        intersection = np.sum(true_class * pred_class)\n",
    "        union = np.sum(true_class) + np.sum(pred_class) - intersection\n",
    "        iou = intersection / union\n",
    "        class_iou.append(iou)\n",
    "        file.write(\"IoU for class {}: {:.2f}\\n\".format(i+1, iou))\n",
    "        print(\"IoU for class {}: {:.2f}\".format(i+1, iou))\n",
    "# Calculate and save average IoU\n",
    "average_iou = np.mean(class_iou)\n",
    "with open(file_path, 'a') as file:\n",
    "    file.write(\"Average IoU: {:.2f}\".format(average_iou))\n",
    "    print(\"Average IoU: {:.2f}\".format(average_iou))\n",
    "print('Saved IoU results')\n",
    "#-------------------------xxxxxx---------------------------------------#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
